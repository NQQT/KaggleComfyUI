{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import threading\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import mmap\n",
    "from threading import Lock\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class BrutalProgressTracker:\n",
    "    def __init__(self, total_size):\n",
    "        self.total_size = total_size\n",
    "        self.downloaded = 0\n",
    "        self.lock = Lock()\n",
    "        self.start_time = time.time()\n",
    "        self.last_update = 0\n",
    "\n",
    "    def update(self, bytes_downloaded):\n",
    "        with self.lock:\n",
    "            self.downloaded += bytes_downloaded\n",
    "            # Update even less frequently - every 250ms\n",
    "            now = time.time()\n",
    "            if now - self.last_update > 0.25:\n",
    "                self._print_progress()\n",
    "                self.last_update = now\n",
    "\n",
    "    def _print_progress(self):\n",
    "        if self.total_size == 0:\n",
    "            return\n",
    "\n",
    "        progress = (self.downloaded / self.total_size) * 100\n",
    "        elapsed = time.time() - self.start_time\n",
    "        speed = self.downloaded / elapsed if elapsed > 0 else 0\n",
    "        speed_mb = speed / (1024 * 1024)\n",
    "\n",
    "        print(f\"\\r{progress:.1f}% | {speed_mb:.2f} MB/s | {self.downloaded//(1024*1024)}/{self.total_size//(1024*1024)} MB\", end='', flush=True)\n",
    "\n",
    "def download_chunk_massive(session, url, start_byte, end_byte, output_path, part_index, progress_tracker):\n",
    "    \"\"\"Downloads with massive chunks and minimal overhead.\"\"\"\n",
    "    headers = {'Range': f'bytes={start_byte}-{end_byte}'}\n",
    "\n",
    "    for attempt in range(2):  # Only 2 attempts, fail fast\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, stream=True, timeout=60)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with open(f\"{output_path}.part{part_index}\", 'wb') as f:\n",
    "                # MASSIVE chunks - 1MB at a time\n",
    "                for chunk in response.iter_content(chunk_size=1048576):  # 1MB chunks\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        progress_tracker.update(len(chunk))\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt == 1:\n",
    "                print(f\"\\nPart {part_index} FAILED: {e}\")\n",
    "                return False\n",
    "            time.sleep(0.1)\n",
    "\n",
    "def create_beast_session():\n",
    "    \"\"\"Creates the most aggressive session possible.\"\"\"\n",
    "    session = requests.Session()\n",
    "\n",
    "    # Maximum aggression\n",
    "    adapter = requests.adapters.HTTPAdapter(\n",
    "        pool_connections=200,\n",
    "        pool_maxsize=200,\n",
    "        max_retries=1,  # Fail fast\n",
    "        pool_block=False\n",
    "    )\n",
    "\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': '*/*',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Accept-Encoding': 'identity'  # No compression for max speed\n",
    "    })\n",
    "\n",
    "    return session\n",
    "\n",
    "def lightning_merge(output_filename, num_parts):\n",
    "    \"\"\"Ultra-fast merge using memory mapping.\"\"\"\n",
    "    print(\"Lightning merge starting...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Calculate total size first\n",
    "    total_size = 0\n",
    "    part_sizes = []\n",
    "    for i in range(num_parts):\n",
    "        part_file = f\"{output_filename}.part{i}\"\n",
    "        if os.path.exists(part_file):\n",
    "            size = os.path.getsize(part_file)\n",
    "            part_sizes.append(size)\n",
    "            total_size += size\n",
    "        else:\n",
    "            print(f\"MISSING PART: {part_file}\")\n",
    "            return False\n",
    "\n",
    "    # Pre-allocate the final file\n",
    "    with open(output_filename, 'wb') as f:\n",
    "        f.seek(total_size - 1)\n",
    "        f.write(b'\\0')\n",
    "\n",
    "    # Memory map the output file\n",
    "    with open(output_filename, 'r+b') as f:\n",
    "        with mmap.mmap(f.fileno(), 0) as mmapped_file:\n",
    "            offset = 0\n",
    "\n",
    "            for i in range(num_parts):\n",
    "                part_file = f\"{output_filename}.part{i}\"\n",
    "\n",
    "                # Copy entire part file at once\n",
    "                with open(part_file, 'rb') as part:\n",
    "                    data = part.read()\n",
    "                    mmapped_file[offset:offset + len(data)] = data\n",
    "                    offset += len(data)\n",
    "\n",
    "                # Delete immediately after copying\n",
    "                os.remove(part_file)\n",
    "\n",
    "    merge_time = time.time() - start_time\n",
    "    print(f\"Merge completed in {merge_time:.2f}s\")\n",
    "    return True\n",
    "\n",
    "def beast_downloader(url, output_filename, num_threads=64, chunk_size_mb=10):\n",
    "    \"\"\"The most aggressive downloader possible.\"\"\"\n",
    "\n",
    "    session = create_beast_session()\n",
    "\n",
    "    try:\n",
    "        response = session.head(url, allow_redirects=True, timeout=15)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        final_url = response.url\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "        if total_size == 0:\n",
    "            print(\"No content-length. Using single thread.\")\n",
    "            return simple_beast_download(session, final_url, output_filename)\n",
    "\n",
    "        if 'accept-ranges' not in response.headers:\n",
    "            print(\"No range support. Single thread.\")\n",
    "            return simple_beast_download(session, final_url, output_filename)\n",
    "\n",
    "        print(f\"Target: {total_size / (1024*1024):.2f} MB\")\n",
    "        print(f\"Unleashing {num_threads} threads with {chunk_size_mb}MB chunks...\")\n",
    "\n",
    "        progress_tracker = BrutalProgressTracker(total_size)\n",
    "\n",
    "        # MASSIVE chunks - minimum 10MB per chunk\n",
    "        min_chunk_size = chunk_size_mb * 1024 * 1024\n",
    "        chunk_size = max(total_size // num_threads, min_chunk_size)\n",
    "        actual_threads = min(num_threads, (total_size + chunk_size - 1) // chunk_size)\n",
    "\n",
    "        print(f\"Using {actual_threads} threads, {chunk_size//(1024*1024)}MB per chunk\")\n",
    "\n",
    "        futures = []\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=actual_threads) as executor:\n",
    "            for i in range(actual_threads):\n",
    "                start_byte = i * chunk_size\n",
    "                end_byte = min(start_byte + chunk_size - 1, total_size - 1)\n",
    "\n",
    "                if start_byte >= total_size:\n",
    "                    break\n",
    "\n",
    "                thread_session = create_beast_session()\n",
    "                future = executor.submit(\n",
    "                    download_chunk_massive,\n",
    "                    thread_session, final_url, start_byte, end_byte,\n",
    "                    output_filename, i, progress_tracker\n",
    "                )\n",
    "                futures.append(future)\n",
    "\n",
    "            # Wait and check\n",
    "            failed = 0\n",
    "            for future in as_completed(futures):\n",
    "                if not future.result():\n",
    "                    failed += 1\n",
    "\n",
    "        if failed > 0:\n",
    "            print(f\"\\n{failed} parts FAILED. Aborting.\")\n",
    "            return False\n",
    "\n",
    "        print(f\"\\nDownload done. Starting lightning merge...\")\n",
    "\n",
    "        # Lightning-fast merge\n",
    "        success = lightning_merge(output_filename, len(futures))\n",
    "\n",
    "        if success:\n",
    "            total_time = time.time() - progress_tracker.start_time\n",
    "            avg_speed = (total_size / total_time) / (1024 * 1024)\n",
    "            print(f\"COMPLETE: {output_filename}\")\n",
    "            print(f\"Total time: {total_time:.1f}s | Average: {avg_speed:.2f} MB/s\")\n",
    "\n",
    "        return success\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FAILED: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        session.close()"
   ],
   "id": "669a0c7d1a0b2b10"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
